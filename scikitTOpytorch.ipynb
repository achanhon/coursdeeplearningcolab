{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCKReHmUvvbBcVM1pxLubX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achanhon/coursdeeplearningcolab/blob/master/scikitTOpytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# scikit-learn vers pytorch\n",
        "\n",
        "L'objet de ce petit notebook est de montrer comment on peut implémenter les MLP de scikit-learn en pytorch (sachant que pytorch permet d'utiliser n'importe quel type de réseau de neurones et pas nécessairement des MLP).\n",
        "\n",
        "Le code du MLP pytorch est bien proche de celui de scikit-learning https://github.com/scikit-learn/scikit-learn/tree/main/sklearn/neural_network (mais il y a des choses différentes comme le raisonnement en epoch etc).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KOPAKmVqe9Xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MNIST\n",
        "Je propose d'utiliser le jeu de données MNIST pour effectuer la comparaison.\n",
        "\n",
        "Pour cela, récupérons le dataset à l'aide de torchvision, puis convertissons le en format vectoriel pour scikit-learn.\n",
        "\n",
        "(Notons, que le dataset torchvision peut directement être donnée à un dataloader torchvision qui fait des paquets - ce qui est pertinent vu que l'optimisation n'est pas globale - cependant c'est le format scikit qui veut ça.)"
      ],
      "metadata": {
        "id": "xD4Is24vg8Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import torch\n",
        "import torchvision\n",
        "import sklearn\n",
        "\n",
        "mnisttrain = torchvision.datasets.MNIST(\"./mnist\",train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "mnisttest = torchvision.datasets.MNIST(\"./mnist\",train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "\n",
        "Xtrain,Ytrain = torch.zeros(len(mnisttrain),28,28),torch.zeros(len(mnisttrain))\n",
        "Xtest,Ytest = torch.zeros(len(mnisttest),28,28),torch.zeros(len(mnisttest))\n",
        "\n",
        "for i in range(Xtrain.shape[0]):\n",
        "  Xtrain[i],Ytrain[i] = mnisttrain[i]\n",
        "for i in range(Xtest.shape[0]):\n",
        "  Xtest[i],Ytest[i] = mnisttest[i]\n",
        "\n",
        "Xtrain,Xtest = Xtrain.flatten(1),Xtest.flatten(1)\n"
      ],
      "metadata": {
        "id": "l4RqFC7UhOzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MNIST avec scikit-learn"
      ],
      "metadata": {
        "id": "Q406t6pRgtlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzH3Zr5Xef22",
        "outputId": "c86ae82c-ec7a-40bd-f6c0-e8db0fd978c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9802"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(random_state=1, max_iter=300)\n",
        "clf.fit(Xtrain.numpy(),Ytrain.numpy())\n",
        "clf.score(Xtest.numpy(),Ytest.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Même chose avec pytorch\n"
      ],
      "metadata": {
        "id": "mKb1-NdSoeAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self,hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, nbepoches=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000):\n",
        "    super(MLP, self).__init__()\n",
        "    self.hidden_layer_sizes = hidden_layer_sizes\n",
        "    self.activation = activation\n",
        "    self.solver = solver\n",
        "    self.alpha = alpha\n",
        "    self.batch_size = 'auto'\n",
        "    self.learning_rate = learning_rate\n",
        "    self.learning_rate_init=learning_rate_init\n",
        "    self.power_t=power_t\n",
        "    self.nbepoches=nbepoches\n",
        "    self.shuffle=shuffle\n",
        "    self.random_state=random_state\n",
        "    self.tol=tol\n",
        "    self.verbose=verbose\n",
        "    self.warm_start=warm_start\n",
        "    self.momentum=momentum\n",
        "    self.nesterovs_momentum=nesterovs_momentum\n",
        "    self.early_stopping=early_stopping\n",
        "    self.validation_fraction=validation_fraction\n",
        "    self.beta_1=beta_1\n",
        "    self.beta_2=beta_2\n",
        "    self.epsilon=epsilon\n",
        "    self.n_iter_no_change=n_iter_no_change\n",
        "    self.max_fun=max_fun\n",
        "\n",
        "    if activation!='relu' or solver!='adam':\n",
        "      print(\"ben faut le faire XD\")\n",
        "      quit()\n",
        "    self.linears = None # be defined after\n",
        "\n",
        "  def forward(self,x):\n",
        "    for i in range(len(self.linears)):\n",
        "      x = self.linears[i](x)\n",
        "      x = torch.nn.functional.leaky_relu(x)\n",
        "    return x\n",
        "\n",
        "  def fit(self,X,Y):\n",
        "    nbclass = int(torch.max(Y).item())+1\n",
        "    if self.verbose:\n",
        "      print(\"nbclass\",nbclass)\n",
        "    dim = X.shape[1]\n",
        "    if self.verbose:\n",
        "      print(\"dim\",dim)\n",
        "\n",
        "    if isinstance(self.hidden_layer_sizes,int):\n",
        "      self.hidden_layer_sizes = [self.hidden_layer_sizes]\n",
        "    self.linears = torch.nn.ModuleList([torch.nn.Linear(dim, self.hidden_layer_sizes[0])])\n",
        "    for i in range(1,len(self.hidden_layer_sizes)):\n",
        "      self.linears.append(torch.nn.Linear(self.hidden_layer_sizes[i-1], self.hidden_layer_sizes[i]))\n",
        "    self.linears.append(torch.nn.Linear(self.hidden_layer_sizes[-1], nbclass))\n",
        "\n",
        "    solver =torch.optim.Adam(self.parameters(), lr=self.learning_rate_init)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(X,Y)\n",
        "    if isinstance(self.batch_size, str):\n",
        "      self.batch_size = 512 # ;-)\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=self.batch_size, shuffle=self.shuffle)\n",
        "    for tmp in range(self.nbepoches):\n",
        "      if self.verbose:\n",
        "        print(tmp)\n",
        "      for x,y in dataloader:\n",
        "        z = self.forward(x)\n",
        "        loss = criterion(z, y.long())\n",
        "        solver.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.parameters(), 10)\n",
        "        solver.step()\n",
        "\n",
        "  def predict(self,X):\n",
        "    with torch.no_grad():\n",
        "      Z = self.forward(X) #assume it will not explode ram...\n",
        "      _,Z = torch.max(Z,dim=1)\n",
        "      return Z\n",
        "\n",
        "  def score(self,X,Y):\n",
        "    Z = self.predict(X)\n",
        "    return torch.sum(Z==Y)/Y.shape[0]\n"
      ],
      "metadata": {
        "id": "fRTLIWbjohv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = MLP(random_state=1, nbepoches=30)\n",
        "clf.fit(Xtrain,Ytrain)\n",
        "clf.score(Xtest,Ytest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FAB6OnJo9He",
        "outputId": "dda42f75-549e-41d8-890d-e7448a375d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9767)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La performance est légèrement moins bonne mais on n'a fait que 30 epoques et pas de validation - cependant, l'esprit est là - et l'idée est de toute façon d'utiliser pytorch pour aller sur des architectures plus générale : avec des produits, avec des résidues, avec des normalisations etc...\n",
        "\n",
        "**Dans l'esprit** :\n",
        "\n",
        "*   Le coeur d'un programme pytorch c'est le *fit* qui n'est donc jamais une simple méthode du réseau mais bien le coeur du code - cela permet d'afficher la courbe de la loss (et donc vérifier que ça diverge pas) de faire une vraie validation (avec éventuellement optimisation d'hyper paramètre)\n",
        "*   Par ailleurs, les principaux *lieux d'actions* pour améliorer la performance dans un tel exemple de classification (hormis l'architecture du réseau) sont : la fonction de perte avec ou sans regularisation et les data augmentations qui sont des éléments pas directement accessibles avec scikit-learn.\n",
        "\n"
      ],
      "metadata": {
        "id": "c6WtbONg3n69"
      }
    }
  ]
}