{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMj53X96C/5rpzbKUfRmyzE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achanhon/coursdeeplearningcolab/blob/master/scikitTOpytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# scikit-learn vers pytorch\n",
        "\n",
        "L'objet de ce petit notebook est de montrer comment on peut implémenter les MLP de scikit-learn en pytorch (sachant que pytorch permet d'utiliser n'importe quel type de réseau de neurones et pas nécessairement des MLP).\n",
        "\n",
        "Normalement, les différences de comportement sont uniquements du à l'aléatoire car la réimplémentation pytorch est basé sur la code source du MLP de scikit-learning https://github.com/scikit-learn/scikit-learn/tree/main/sklearn/neural_network .\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KOPAKmVqe9Xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MNIST\n",
        "Je propose d'utiliser le jeu de données MNIST pour effectuer la comparaison.\n",
        "\n",
        "Pour cela, récupérons le dataset à l'aide de torchvision, puis convertissons le en format vectoriel pour scikit-learn.\n",
        "\n",
        "(Notons, que le dataset torchvision peut directement être donnée à un dataloader torchvision qui fait des paquets - ce qui est pertinent vu que l'optimisation n'est pas globale - cependant c'est le format scikit qui veut ça.)"
      ],
      "metadata": {
        "id": "xD4Is24vg8Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import torch\n",
        "import torchvision\n",
        "import sklearn\n",
        "\n",
        "mnisttrain = torchvision.datasets.MNIST(\"./mnist\",train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "mnisttest = torchvision.datasets.MNIST(\"./mnist\",train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "\n",
        "Xtrain,Ytrain = torch.zeros(len(mnisttrain),28,28),torch.zeros(len(mnisttrain))\n",
        "Xtest,Ytest = torch.zeros(len(mnisttest),28,28),torch.zeros(len(mnisttest))\n",
        "\n",
        "for i in range(Xtrain.shape[0]):\n",
        "  Xtrain[i],Ytrain[i] = mnisttrain[i]\n",
        "for i in range(Xtest.shape[0]):\n",
        "  Xtest[i],Ytest[i] = mnisttest[i]\n",
        "\n",
        "Xtrain,Xtest = Xtrain.flatten(1),Xtest.flatten(1)\n"
      ],
      "metadata": {
        "id": "l4RqFC7UhOzA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MNIST avec scikit-learn"
      ],
      "metadata": {
        "id": "Q406t6pRgtlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzH3Zr5Xef22",
        "outputId": "1e930b80-fa92-438b-f60a-e7bad7379bd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(max_iter=300, random_state=1)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(random_state=1, max_iter=300)\n",
        "clf.fit(Xtrain,Ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(Xtest.numpy(),Ytest.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuRsITdfoPnQ",
        "outputId": "57ec68d8-a633-45db-b5fe-4675eb444fe1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9792"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Même chose avec pytorch\n"
      ],
      "metadata": {
        "id": "mKb1-NdSoeAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self,hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000):\n",
        "    super(MLP, self).__init__()\n",
        "    self.hidden_layer_sizes = hidden_layer_sizes\n",
        "    self.activation = activation\n",
        "    self.solver = solver\n",
        "    self.alpha = alpha\n",
        "    self.batch_size = 'auto'\n",
        "    self.learning_rate = learning_rate\n",
        "    self.learning_rate_init=learning_rate_init\n",
        "    self.power_t=power_t\n",
        "    self.max_iter=max_iter\n",
        "    self.shuffle=shuffle\n",
        "    self.random_state=random_state\n",
        "    self.tol=tol\n",
        "    self.verbose=verbose\n",
        "    self.warm_start=warm_start\n",
        "    self.momentum=momentum\n",
        "    self.nesterovs_momentum=nesterovs_momentum\n",
        "    self.early_stopping=early_stopping\n",
        "    self.validation_fraction=validation_fraction\n",
        "    self.beta_1=beta_1\n",
        "    self.beta_2=beta_2\n",
        "    self.epsilon=epsilon\n",
        "    self.n_iter_no_change=n_iter_no_change\n",
        "    self.max_fun=max_fun\n",
        "\n",
        "    if activation!='relu' or solver!='adam':\n",
        "      print(\"ben faut le faire XD\")\n",
        "      quit()\n",
        "\n",
        "  def forward(self,x):\n",
        "    for i in range(len(self.linears)):\n",
        "      x = self.linears[i](x)\n",
        "      x = torch.nn.functional.leaky_relu(x)\n",
        "    return x\n",
        "\n",
        "  def fit(self,X,Y):\n",
        "    nbclass = torch.max(Y)\n",
        "    dim = X.shape[1]\n",
        "\n",
        "    self.linears = torch.nn.ModuleList([torch.nn.Linear(dim, self.hidden_layer_sizes[0])])\n",
        "    for i in range(1,len(self.hidden_layer_sizes[0])):\n",
        "      self.linears.append(torch.nn.Linear(self.hidden_layer_sizes[i-1], self.hidden_layer_sizes[i]))\n",
        "    self.linears.append(torch.nn.Linear(self.hidden_layer_sizes[-1], nbclass))\n",
        "\n",
        "    self.solver =torch.optim.Adam(self.parameters(), lr=self.learning_rate_init)\n",
        "    self.criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(X,Y)\n",
        "    if isinstance(self.batch_size, str):\n",
        "      self.batch_size = 128 # ;-)\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=64, shuffle=True)\n",
        "\n",
        "  def predict(self,X):\n",
        "\n",
        "  def score(self,X,Y):\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "fRTLIWbjohv3",
        "outputId": "64f61034-1704-469d-f082-c603812502bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-fb549926a679>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class MLP(torch.nn.Module):\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1FAB6OnJo9He"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}