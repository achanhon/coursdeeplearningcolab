{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSkKopNWPz7mnU45LOgerB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achanhon/coursdeeplearningcolab/blob/master/rl_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\"\n",
        "!pip install schedulefree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rloprxr9IZbr",
        "outputId": "b09c10a8-97ea-4864-a6f1-88707b09f8d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376102 sha256=b709e18532d1823f4f117889172f6a55608d0f3ee4b57a4d0c2db2bc7b94dae8\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n",
            "Collecting schedulefree\n",
            "  Downloading schedulefree-1.2.7.tar.gz (19 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: schedulefree\n",
            "  Building wheel for schedulefree (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for schedulefree: filename=schedulefree-1.2.7-py3-none-any.whl size=26782 sha256=c4550025b8f40f64f511e76b7fe934083020d2f2c6f45e29552ecab8fb72d83a\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/b9/d6/035dcef7bbf02667237a5cc451d0d8d43994a2d3256d113ed9\n",
            "Successfully built schedulefree\n",
            "Installing collected packages: schedulefree\n",
            "Successfully installed schedulefree-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K4RF9azGJ4RN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gymnasium\n",
        "import schedulefree\n",
        "\n",
        "GAMMA=0.99\n",
        "\n",
        "class MemoryBuffer:\n",
        "    def __init__(self):\n",
        "        self.i = 0\n",
        "        self.full = False\n",
        "\n",
        "        L=10000\n",
        "        self.s = torch.zeros(L,8)\n",
        "        self.a = torch.zeros(L,4)\n",
        "        self.r = torch.zeros(L)\n",
        "        self.s_ = torch.zeros(L,8)\n",
        "        self.f = torch.zeros(L)\n",
        "\n",
        "    def push(self, s, a, r, s_, f):\n",
        "        self.s[self.i] = s\n",
        "        self.a[self.i][a] = 1\n",
        "        self.r[self.i] = r\n",
        "        self.s_[self.i] = s_\n",
        "        self.f[self.i] = 1-f\n",
        "        self.i += 1\n",
        "        if self.i >= self.r.shape[0]:\n",
        "            self.full = True\n",
        "            self.i = 0\n",
        "\n",
        "    def getBatch(self, B=64):\n",
        "        if self.full:\n",
        "            I = list((torch.rand(B) * self.r.shape[0]).long())\n",
        "        else:\n",
        "            I = list((torch.rand(B) * self.i).long())\n",
        "        return (self.s[I], self.a[I], self.r[I], self.s_[I], self.f[I])\n",
        "\n",
        "def trial(env,agent, T, memory):\n",
        "    totalR = 0\n",
        "    s, info = env.reset()\n",
        "    s = torch.Tensor(s)\n",
        "    for _ in range(1000):\n",
        "        a = agent.sample(s,T)\n",
        "        s_, r, terminated, truncated, info = env.step(a)\n",
        "        s_ = torch.Tensor(s_)\n",
        "\n",
        "        memory.push(s,a,r,s_,terminated or truncated)\n",
        "        totalR+=r\n",
        "        if terminated or truncated:\n",
        "            return totalR\n",
        "        else:\n",
        "            s = s_\n",
        "\n",
        "def train(agent,T,memory,nbstep):\n",
        "    optimizer = schedulefree.AdamWScheduleFree(agent.parameters(), lr=0.001)\n",
        "\n",
        "    meanloss = torch.zeros(nbstep)\n",
        "    for step in range(nbstep):\n",
        "        B = memory.getBatch()\n",
        "        S, A, R, S_,F = B\n",
        "\n",
        "        Q = agent.Q(S)\n",
        "        QA = (Q * A).sum(1)\n",
        "        Q_ = agent.V(S_,T)\n",
        "        loss = ((GAMMA * Q_ * F + R - QA)**2).mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            meanloss[step] = loss.clone()\n",
        "    return float(meanloss.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def leakyRelu(x):\n",
        "    return torch.minimum(x,x*0.2)\n",
        "\n",
        "class Block(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Block,self).__init__()\n",
        "\n",
        "        self.f1 =torch.nn.Linear(24,8)\n",
        "        self.f2 =torch.nn.Linear(8,24)\n",
        "        self.f3 =torch.nn.Linear(24,16)\n",
        "\n",
        "    def forward(self,x):\n",
        "        f = leakyRelu(self.f1(x))\n",
        "        f = leakyRelu(self.f2(f))\n",
        "        f = leakyRelu(self.f3(f))\n",
        "\n",
        "        tmp = torch.zeros(x.shape[0],8)\n",
        "        f = torch.cat([tmp,f],dim=1)\n",
        "        return x+f\n",
        "\n",
        "\n",
        "class LunarAgent(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LunarAgent,self).__init__()\n",
        "\n",
        "        self.b1 = Block()\n",
        "        self.b2 = Block()\n",
        "        self.b3 = Block()\n",
        "        self.b4 = Block()\n",
        "        self.b5 = Block()\n",
        "        self.b6 = Block()\n",
        "        self.b7 = Block()\n",
        "\n",
        "        self.A =torch.nn.Linear(24,4)\n",
        "\n",
        "    def forward(self,x):\n",
        "        code = torch.zeros(x.shape[0],16)\n",
        "        x = torch.cat([x,code],dim=1)\n",
        "\n",
        "        x = self.b1(x)\n",
        "        x = self.b2(x)\n",
        "        x = self.b3(x)\n",
        "        x = self.b4(x)\n",
        "        x = self.b5(x)\n",
        "        x = self.b6(x)\n",
        "        x = self.b7(x)\n",
        "\n",
        "        return self.A(x)\n",
        "\n",
        "    def Q(self,x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def pi(self,Q,T):\n",
        "        return torch.nn.functional.softmax(T*Q,dim=1)\n",
        "\n",
        "    def V(self,x,T):\n",
        "        Q =self.Q(x)\n",
        "        pi = self.pi(Q,T)\n",
        "        return (Q*pi).sum(1)\n",
        "\n",
        "    def sample(self,x,T):\n",
        "        with torch.no_grad():\n",
        "            pi = self.pi(self.Q(x.view(1,-1)),T)\n",
        "            return int(torch.multinomial(pi, num_samples=1))"
      ],
      "metadata": {
        "id": "b3Bk10nV2gYi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gymnasium.make(\"LunarLander-v3\", continuous=False, gravity=-8.0,enable_wind=False)\n",
        "\n",
        "T = 0.25\n",
        "agent = LunarAgent()\n",
        "\n",
        "for j in range(10):\n",
        "    memory = MemoryBuffer()\n",
        "    for _ in range(50):\n",
        "        trial(env,agent,T,memory)\n",
        "    for i in range(20+j*5):\n",
        "        v = trial(env,agent,T,memory)\n",
        "        l = train(agent,T,memory, nbstep=100+30*j)\n",
        "        print(\"\\t\",v,l)\n",
        "    tot,tot_ = 0,0\n",
        "    for _ in range(100):\n",
        "        tot+=trial(env,agent,T,memory)\n",
        "        tot_+=trial(env,agent,T*1.3,memory)\n",
        "    print(T,tot/100,tot_/100)\n",
        "    if tot<tot_:\n",
        "        T = 1.3*T\n"
      ],
      "metadata": {
        "id": "pgzIHX3nNIWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4ce275-7610-4fda-ea2b-3a118d3998ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t -345.7655706821955 102.96977233886719\n",
            "\t -97.91735229908537 98.11039733886719\n",
            "\t -437.0885561939049 69.7258071899414\n",
            "\t -433.4941312467837 19.52145767211914\n",
            "\t -411.1011152998981 13.909363746643066\n",
            "\t -203.8995195297377 9.448213577270508\n",
            "\t -65.91881972040338 10.514863014221191\n",
            "\t -311.38980717559537 10.197474479675293\n",
            "\t -14.379819660352013 10.75926399230957\n",
            "\t -85.24373970556735 9.943022727966309\n",
            "\t -34.22665250400922 10.828673362731934\n",
            "\t -66.65336622996215 9.050848007202148\n",
            "\t -198.12926428258015 8.090961456298828\n",
            "\t -115.49384909896432 8.015170097351074\n",
            "\t -102.02303909749712 6.289062023162842\n",
            "\t -62.383052718819876 6.777736663818359\n",
            "\t -179.5925827965764 7.692903518676758\n",
            "\t -326.34773929284347 7.591897487640381\n",
            "\t -118.3567101672956 6.818421840667725\n",
            "\t -311.04607371871026 7.7490925788879395\n",
            "0.25 -143.20862136961628 -133.82640996549972\n",
            "\t -217.16701896427233 7.110373020172119\n",
            "\t -18.852689404791988 6.676052570343018\n",
            "\t -146.80801904464295 7.142418384552002\n",
            "\t -232.4611199049043 7.04820442199707\n",
            "\t -110.18834921885212 5.5321946144104\n",
            "\t -45.519738756400706 4.6367669105529785\n"
          ]
        }
      ]
    }
  ]
}